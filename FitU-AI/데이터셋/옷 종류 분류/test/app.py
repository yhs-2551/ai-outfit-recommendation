import streamlit as st
import torch
import timm
import numpy as np
from PIL import Image
import torchvision.transforms as transforms
from ultralytics import YOLO
import cv2
import os
import tempfile
import requests
import base64
from datetime import datetime
import time
from dotenv import load_dotenv
from pathlib import Path

# .env íŒŒì¼ ë¡œë“œ (ìƒìœ„ ë””ë ‰í† ë¦¬ì˜ .env íŒŒì¼ ì‚¬ìš©)
env_path = Path(__file__).parents[3] / '.env'
load_dotenv(dotenv_path=env_path)

# ìºì‹œ ì„¤ì •
@st.cache_resource
def load_classification_model():
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = timm.create_model('efficientnetv2_s', pretrained=False, num_classes=3)
    model.load_state_dict(torch.load("ë°ì´í„°ì…‹/ì˜· ì¢…ë¥˜ ë¶„ë¥˜/model/classification_efficientnetv2_s.pt", map_location=DEVICE))
    model.to(DEVICE)
    model.eval()
    return model, DEVICE

@st.cache_resource
def load_pattern_model():
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    checkpoint = torch.load('ë°ì´í„°ì…‹\ì˜· ì¢…ë¥˜ ë¶„ë¥˜\model\clothes_pattern.pt', map_location=DEVICE)
    class_names = checkpoint['class_names']
    num_classes = len(class_names)
    
    model = timm.create_model('efficientnetv2_rw_s', pretrained=False, num_classes=num_classes)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.to(DEVICE)
    model.eval()
    return model, class_names, DEVICE

@st.cache_resource
def load_segmentation_model(model_path):
    return YOLO(model_path)

def analyze_color_tone(image):
    """ì´ë¯¸ì§€ì˜ ìƒ‰ìƒ í†¤ì„ ë¶„ì„í•˜ëŠ” í•¨ìˆ˜"""
    # í°ìƒ‰ ë°°ê²½(255, 255, 255)ì„ ì œì™¸í•œ í”½ì…€ë§Œ ì¶”ì¶œ
    mask = ~np.all(image == 255, axis=2)
    clothing_pixels = image[mask]
    
    if len(clothing_pixels) == 0:
        return "ë¶„ì„ ë¶ˆê°€", 0
    
    # RGB í‰ê· ê°’ ê³„ì‚°
    avg_rgb = np.mean(clothing_pixels, axis=0)
    
    # RGBë¥¼ HSVë¡œ ë³€í™˜
    avg_rgb_normalized = avg_rgb / 255.0
    avg_rgb_normalized = avg_rgb_normalized.reshape(1, 1, 3)
    avg_hsv = cv2.cvtColor(avg_rgb_normalized.astype(np.float32), cv2.COLOR_RGB2HSV)
    value = avg_hsv[0, 0, 2] * 255  # ëª…ë„ ê°’ (0-255)
    
    # í†¤ ë¶„ë¥˜
    if value > 128:
        tone = "ë°ì€ í†¤"
    else:
        tone = "ì–´ë‘ìš´ í†¤"
    
    return tone, value

def process_segmentation(image_path, model_path):
    # YOLO ëª¨ë¸ ë¡œë“œ
    model = load_segmentation_model(model_path)
    
    # ì˜ˆì¸¡ ìˆ˜í–‰
    results = model(image_path)
    
    # ì›ë³¸ ì´ë¯¸ì§€ ì½ê¸°
    original_image = cv2.imread(image_path)
    original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)  # BGRì„ RGBë¡œ ë³€í™˜
    
    # ê²°ê³¼ ì²˜ë¦¬
    for result in results:
        if result.masks is not None:
            # ë¶„ë¥˜ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
            class_names = result.names
            boxes = result.boxes
            class_ids = boxes.cls.cpu().numpy()
            confidences = boxes.conf.cpu().numpy()
            
            # ë§ˆìŠ¤í¬ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            masks = result.masks.data.cpu().numpy()
            
            # ëª¨ë“  ë§ˆìŠ¤í¬ë¥¼ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°
            combined_mask = np.zeros_like(masks[0], dtype=np.uint8)
            for mask in masks:
                combined_mask = np.logical_or(combined_mask, mask)
            
            # ë§ˆìŠ¤í¬ë¥¼ ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
            original_height, original_width = original_image.shape[:2]
            resized_mask = cv2.resize(combined_mask.astype(np.uint8), 
                                    (original_width, original_height), 
                                    interpolation=cv2.INTER_NEAREST)
            
            # ë§ˆìŠ¤í¬ë¥¼ 3ì±„ë„ë¡œ í™•ì¥
            mask_3channel = np.stack([resized_mask] * 3, axis=-1)
            
            # ë§ˆìŠ¤í¬ ì ìš© (ì˜· ë¶€ë¶„ë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ëŠ” í°ìƒ‰ìœ¼ë¡œ)
            segmented_image = np.where(mask_3channel, original_image, 255)
            
            # ìƒ‰ìƒ í†¤ ë¶„ì„ (ì„¸ê·¸ë©˜í…Œì´ì…˜ ì§í›„, ì´ë¯¸ì§€ ë³€í™˜ ì „ì— ìˆ˜í–‰)
            tone, value = analyze_color_tone(segmented_image)
            
            # ë§ˆìŠ¤í¬ì˜ ê²½ê³„ ìƒì ì°¾ê¸°
            y_indices, x_indices = np.where(resized_mask)
            if len(y_indices) > 0 and len(x_indices) > 0:
                y_min, y_max = y_indices.min(), y_indices.max()
                x_min, x_max = x_indices.min(), x_indices.max()
                
                # íŒ¨ë”© ì¶”ê°€ (ì´ë¯¸ì§€ í¬ê¸°ì˜ 10%)
                padding_y = int((y_max - y_min) * 0.1)
                padding_x = int((x_max - x_min) * 0.1)
                
                # íŒ¨ë”©ì„ ì ìš©í•œ ìƒˆë¡œìš´ ê²½ê³„ ìƒì ê³„ì‚°
                y_min = max(0, y_min - padding_y)
                y_max = min(original_height - 1, y_max + padding_y)
                x_min = max(0, x_min - padding_x)
                x_max = min(original_width - 1, x_max + padding_x)
                
                # ì˜· ë¶€ë¶„ë§Œ í¬ë¡­
                cropped_image = segmented_image[y_min:y_max+1, x_min:x_max+1]
                
                # 640x640 í°ìƒ‰ ë°°ê²½ ìƒì„±
                final_image = np.ones((640, 640, 3), dtype=np.uint8) * 255
                
                # í¬ë¡­ëœ ì´ë¯¸ì§€ì˜ í¬ê¸° ê³„ì‚°
                h, w = cropped_image.shape[:2]
                
                # ì´ë¯¸ì§€ ë¹„ìœ¨ ìœ ì§€í•˜ë©´ì„œ 640x640ì— ë§ê²Œ ë¦¬ì‚¬ì´ì¦ˆ
                scale = min(640/w, 640/h)
                new_w, new_h = int(w * scale), int(h * scale)
                resized_crop = cv2.resize(cropped_image, (new_w, new_h), interpolation=cv2.INTER_AREA)
                
                # ì¤‘ì•™ì— ë°°ì¹˜
                y_offset = (640 - new_h) // 2
                x_offset = (640 - new_w) // 2
                final_image[y_offset:y_offset+new_h, x_offset:x_offset+new_w] = resized_crop
                
                # ë¶„ë¥˜ ê²°ê³¼ ë°˜í™˜
                detection_results = []
                for class_id, confidence in zip(class_ids, confidences):
                    class_name = class_names[int(class_id)]
                    detection_results.append((class_name, float(confidence)))
                
                return final_image, detection_results, tone, value
    return None, [], "ë¶„ì„ ë¶ˆê°€", 0

def predict_pattern(image, model, class_names, device):
    # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                           std=[0.229, 0.224, 0.225])
    ])
    
    # ì´ë¯¸ì§€ ë³€í™˜
    input_tensor = transform(image).unsqueeze(0).to(device)
    
    # ì˜ˆì¸¡
    with torch.no_grad():
        output = model(input_tensor)
        probs = torch.softmax(output, dim=1)
        predicted = torch.argmax(output, dim=1)
        confidence = probs[0][predicted].item()
        predicted_class = class_names[predicted.item()]
    
    return predicted_class, confidence

# FASHN API ì„¤ì •
API_KEY = os.getenv("FASHN_API_KEY")
if not API_KEY:
    st.error("FASHN API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    st.stop()
BASE_URL = "https://api.fashn.ai/v1"

def encode_image_to_base64(image_path):
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')

def download_image(url, save_path):
    response = requests.get(url)
    if response.status_code == 200:
        with open(save_path, 'wb') as f:
            f.write(response.content)
        return True
    return False

def process_virtual_tryon(model_image_path, garment_image_path, category):
    try:
        # ì´ë¯¸ì§€ë¥¼ Base64ë¡œ ì¸ì½”ë”©
        model_image_base64 = encode_image_to_base64(model_image_path)
        garment_image_base64 = encode_image_to_base64(garment_image_path)

        # API ìš”ì²­ ë°ì´í„° ì¤€ë¹„
        input_data = {
            "model_image": f"data:image/jpeg;base64,{model_image_base64}",
            "garment_image": f"data:image/jpeg;base64,{garment_image_base64}",
            "category": category
        }

        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {API_KEY}"
        }

        # 1. /run ì—”ë“œí¬ì¸íŠ¸ë¡œ ìš”ì²­
        run_response = requests.post(f"{BASE_URL}/run", json=input_data, headers=headers)
        run_data = run_response.json()
        
        if run_response.status_code != 200:
            return None, f"API í˜¸ì¶œ ì‹¤íŒ¨: {run_data.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì—ëŸ¬')}"
            
        prediction_id = run_data.get("id")
        if not prediction_id:
            return None, "ì˜ˆì¸¡ IDë¥¼ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤"
            
        # 2. ìƒíƒœ í™•ì¸ ë° ê²°ê³¼ ëŒ€ê¸°
        while True:
            status_response = requests.get(f"{BASE_URL}/status/{prediction_id}", headers=headers)
            if status_response.status_code != 200:
                return None, f"ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {status_response.text}"
                
            status_data = status_response.json()

            if status_data["status"] == "completed":
                result_url = status_data["output"][0] if isinstance(status_data["output"], list) else status_data["output"]
                
                # ê²°ê³¼ ì´ë¯¸ì§€ ì €ì¥
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                output_dir = "ë°ì´í„°ì…‹/ì˜· ì¢…ë¥˜ ë¶„ë¥˜/test/virtual_tryon_result"
                os.makedirs(output_dir, exist_ok=True)
                output_path = os.path.join(output_dir, f"result_{timestamp}.png")
                
                if download_image(result_url, output_path):
                    return output_path, None
                else:
                    return None, "ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨"

            elif status_data["status"] in ["starting", "in_queue", "processing"]:
                time.sleep(3)
            else:
                return None, f"ì˜ˆì¸¡ ì‹¤íŒ¨: {status_data.get('error')}"

    except Exception as e:
        return None, f"ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

def main():
    st.title("ğŸ‘” ì˜ë¥˜ ë¶„ë¥˜, ì„¸ê·¸ë©˜í…Œì´ì…˜ ë° ê°€ìƒ í”¼íŒ… ì‹œìŠ¤í…œ")
    
    # ëª¨ë¸ ì´ë¯¸ì§€ ì—…ë¡œë”
    st.subheader("ğŸ‘¤ ëª¨ë¸ ì´ë¯¸ì§€ ì—…ë¡œë“œ")
    model_image = st.file_uploader("ëª¨ë¸ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”", type=['jpg', 'jpeg', 'png'], key="model")
    
    # ì˜ë¥˜ ì´ë¯¸ì§€ ì—…ë¡œë”
    st.subheader("ğŸ‘• ì˜ë¥˜ ì´ë¯¸ì§€ ì—…ë¡œë“œ")
    garment_image = st.file_uploader("ì˜ë¥˜ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”", type=['jpg', 'jpeg', 'png'], key="garment")
    
    if garment_image is not None:
        # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
        with tempfile.NamedTemporaryFile(delete=False, suffix='.jpg') as tmp_file:
            tmp_file.write(garment_image.getvalue())
            tmp_path = tmp_file.name
        
        # ì›ë³¸ ì´ë¯¸ì§€ í‘œì‹œ
        st.subheader("ğŸ“¸ ì›ë³¸ ì´ë¯¸ì§€")
        st.image(garment_image, use_container_width=True)
        
        # 1. íŒ¨í„´ ë¶„ë¥˜ ìˆ˜í–‰
        st.subheader("ğŸ¨ íŒ¨í„´ ë¶„ë¥˜")
        pattern_model, class_names, pattern_device = load_pattern_model()
        image = Image.open(tmp_path).convert("RGB")
        pattern, confidence = predict_pattern(image, pattern_model, class_names, pattern_device)
        st.write(f"**ì˜ˆì¸¡ëœ íŒ¨í„´:** {pattern}")
        st.write(f"**ì‹ ë¢°ë„:** {confidence:.3f}")
        
        # 2. ë¶„ë¥˜ ëª¨ë¸ ë¡œë“œ ë° ì˜ˆì¸¡
        model, DEVICE = load_classification_model()

        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        transform = transforms.Compose([
            transforms.Resize((640, 640)),
            transforms.ToTensor(),
        ])

        input_tensor = transform(image).unsqueeze(0).to(DEVICE)

        # ì˜ˆì¸¡ ìˆ˜í–‰
        with torch.no_grad():
            output = model(input_tensor)
            probs = torch.sigmoid(output).cpu().numpy()[0]
            preds = (probs > 0.5).astype(int)

        # ë¼ë²¨ ë§¤í•‘
        classes = ['top', 'bottom', 'onepiece']
        results = [(cls, int(p), round(prob, 3)) for cls, p, prob in zip(classes, preds, probs)]

        # ê²°ê³¼ ì¶œë ¥
        st.subheader("ğŸ” ì˜ë¥˜ ìœ í˜• ë¶„ë¥˜ ê²°ê³¼")
        detected_classes = []
        for label, is_present, confidence in results:
            status = "âœ… ìˆìŒ" if is_present else "âŒ ì—†ìŒ"
            st.write(f"{label:<10}: {status} (ì‹ ë¢°ë„: {confidence:.3f})")
            if is_present:
                detected_classes.append(label)

        # 3. ì„¸ê·¸ë©˜í…Œì´ì…˜ ìˆ˜í–‰
        model_paths = {
            'top': "ë°ì´í„°ì…‹/ì˜· ì¢…ë¥˜ ë¶„ë¥˜/model/top_yolo11seg_m.pt",
            'bottom': "ë°ì´í„°ì…‹/ì˜· ì¢…ë¥˜ ë¶„ë¥˜/model/bottom_yolo11seg_m.pt",
            'onepiece': "ë°ì´í„°ì…‹/ì˜· ì¢…ë¥˜ ë¶„ë¥˜/model/onepiece_yolo11seg_m.pt"
        }

        # ì„¸ê·¸ë©˜í…Œì´ì…˜ ê²°ê³¼ í‘œì‹œ
        if detected_classes:
            st.subheader("ğŸ¯ ì„¸ê·¸ë©˜í…Œì´ì…˜ ê²°ê³¼")
            cols = st.columns(len(detected_classes))
            
            segmented_images = []
            for idx, cls in enumerate(detected_classes):
                if cls in model_paths:
                    with cols[idx]:
                        st.write(f"**{cls}**")
                        segmented_image, detection_results, tone, value = process_segmentation(tmp_path, model_paths[cls])
                        if segmented_image is not None:
                            st.image(segmented_image, use_container_width=True)
                            segmented_images.append((cls, segmented_image))
                            
                            # ìƒ‰ìƒ í†¤ ì •ë³´ í‘œì‹œ
                            st.write(f"**ìƒ‰ìƒ í†¤ ë¶„ì„:**")
                            st.write(f"- í†¤: {tone}")
                            st.write(f"- ëª…ë„ ê°’: {value:.1f}")
                            
                            if detection_results:
                                st.write("**YOLO ë¶„ë¥˜ ê²°ê³¼:**")
                                for class_name, confidence in detection_results:
                                    st.write(f"- {class_name}: {confidence:.3f}")
            
            # ê°€ìƒ í”¼íŒ… ì„¹ì…˜
            if model_image is not None and segmented_images:
                st.subheader("ğŸ¨ ê°€ìƒ í”¼íŒ…")
                
                # ëª¨ë¸ ì´ë¯¸ì§€ ì„ì‹œ ì €ì¥
                with tempfile.NamedTemporaryFile(delete=False, suffix='.jpg') as model_tmp:
                    model_tmp.write(model_image.getvalue())
                    model_tmp_path = model_tmp.name
                
                # ì¹´í…Œê³ ë¦¬ ë§¤í•‘
                category_mapping = {
                    'top': 'tops',
                    'bottom': 'bottoms',
                    'onepiece': 'one-pieces'
                }
                
                # ê° ì„¸ê·¸ë©˜í…Œì´ì…˜ ê²°ê³¼ì— ëŒ€í•´ ê°€ìƒ í”¼íŒ… ìˆ˜í–‰
                for cls, seg_image in segmented_images:
                    st.write(f"**{cls} ê°€ìƒ í”¼íŒ… ê²°ê³¼**")
                    
                    # ì„¸ê·¸ë©˜í…Œì´ì…˜ ì´ë¯¸ì§€ ì„ì‹œ ì €ì¥
                    with tempfile.NamedTemporaryFile(delete=False, suffix='.jpg') as seg_tmp:
                        cv2.imwrite(seg_tmp.name, cv2.cvtColor(seg_image, cv2.COLOR_RGB2BGR))
                        seg_tmp_path = seg_tmp.name
                    
                    # ê°€ìƒ í”¼íŒ… ìˆ˜í–‰ (ì¹´í…Œê³ ë¦¬ ë§¤í•‘ ì ìš©)
                    category = category_mapping.get(cls, 'auto')
                    result_path, error = process_virtual_tryon(model_tmp_path, seg_tmp_path, category)
                    
                    if result_path:
                        st.image(result_path, use_container_width=True)
                    else:
                        st.error(error)
                    
                    # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                    os.unlink(seg_tmp_path)
                
                # ëª¨ë¸ ì´ë¯¸ì§€ ì„ì‹œ íŒŒì¼ ì‚­ì œ
                os.unlink(model_tmp_path)
        
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        os.unlink(tmp_path)

if __name__ == "__main__":
    main() 